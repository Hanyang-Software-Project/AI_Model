{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95ae7b86",
      "metadata": {
        "id": "95ae7b86"
      },
      "source": [
        "## Unsupervised Classification of Normal and Anomalous Sounds Using Autoencoder CNN and VAE\n",
        "\n",
        "This script tests the ability of a **Convolutional Autoencoder (CNN)** and a **Variational Autoencoder (VAE)**  \n",
        "to perform unsupervised anomaly detection by reconstructing normal sound data and identifying anomalous sounds  \n",
        "based on reconstruction errors.\n",
        "\n",
        "### Workflow:\n",
        "1. **Training Phase**:  \n",
        "   - The models are trained exclusively on **normal sound data**.\n",
        "2. **Testing Phase**:  \n",
        "   - The models are evaluated on a mix of **normal and anomalous sounds**.\n",
        "\n",
        "### Assumption:\n",
        "- Models trained to reconstruct normal data will produce **higher reconstruction errors** when encountering anomalous data.\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations:\n",
        "- **Dataset Size**: The current training dataset consists of **only 200 normal sound files**.  \n",
        "  Even with data augmentation (about **400 samples**), this is insufficient for effective reconstruction learning.\n",
        "- **Model Complexity**: Autoencoders and VAEs require large, diverse datasets to learn meaningful feature representations.  \n",
        "  A small dataset can lead to **overfitting** and poor performance on unseen data.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d1c40d",
      "metadata": {
        "id": "27d1c40d"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e805b9c5",
      "metadata": {
        "id": "e805b9c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, f1_score, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import os\n",
        "#import cv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aVBw8K_INmyC",
      "metadata": {
        "id": "aVBw8K_INmyC"
      },
      "source": [
        "## Show the gpu infos\n",
        "... and change if needed in \"Execution\" panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ELy3j_VNojG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ELy3j_VNojG",
        "outputId": "7ca6b419-85b7-4e86-fd97-805f88b78028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 13 05:44:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(\"Python version:\", sys.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp-bKaFGTKkx",
        "outputId": "d90d5437-089c-4aa6-cbc6-d81759de0216"
      },
      "id": "Yp-bKaFGTKkx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nl8J-wmEMgPU",
      "metadata": {
        "id": "nl8J-wmEMgPU"
      },
      "source": [
        "## Import the spectrograms from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "D1qNySc7MsH3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1qNySc7MsH3",
        "outputId": "bd15823d-1445-44b3-c549-b844a857e536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import random\n",
        "\n",
        "# Function to add noise\n",
        "def add_noise(data, noise_level=0.005):\n",
        "    noise = np.random.normal(0, noise_level, data.shape)\n",
        "    return data + noise\n",
        "\n",
        "# Function to apply time stretching\n",
        "def time_stretch(data, rate=1.1):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "# Function to apply pitch shifting\n",
        "def pitch_shift(data, sr, n_steps=2):\n",
        "    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n",
        "\n",
        "# Function to adjust volume\n",
        "def adjust_volume(data, factor=1.2):\n",
        "    return data * factor\n",
        "\n",
        "# Function to apply time shifting\n",
        "def time_shift(data, shift_max=0.2):\n",
        "    shift = int(random.uniform(-shift_max, shift_max) * len(data))\n",
        "    return np.roll(data, shift)\n",
        "\n",
        "# Function to create augmentations for a given file\n",
        "def create_augmentations(file_path, output_folder, sr=22050, augmentations=3):\n",
        "    # Load the original audio\n",
        "    data = np.load(file_path)\n",
        "    audio = librosa.feature.inverse.mel_to_audio(data, sr=sr)\n",
        "\n",
        "    # Create augmentations\n",
        "    for i in range(augmentations):\n",
        "        augmented_audio = audio\n",
        "\n",
        "        # Randomly apply augmentations\n",
        "        if random.choice([True, False]):\n",
        "            augmented_audio = add_noise(augmented_audio)\n",
        "        if random.choice([True, False]):\n",
        "            augmented_audio = pitch_shift(augmented_audio, sr, n_steps=random.randint(-2, 2))\n",
        "        if random.choice([True, False]):\n",
        "            augmented_audio = adjust_volume(augmented_audio, factor=random.uniform(0.8, 1.2))\n",
        "        if random.choice([True, False]):\n",
        "            augmented_audio = time_shift(augmented_audio)\n",
        "\n",
        "        # Convert back to Mel spectrogram\n",
        "        augmented_mel = librosa.feature.melspectrogram(y=augmented_audio, sr=sr)\n",
        "\n",
        "        # Save the augmented data\n",
        "        augmented_filename = f\"{os.path.splitext(os.path.basename(file_path))[0]}_aug_{i}.npy\"\n",
        "        augmented_filepath = os.path.join(output_folder, augmented_filename)\n",
        "        np.save(augmented_filepath, augmented_mel)\n",
        "\n",
        "# Path to the normal data folder\n",
        "normal_data_folder = '/content/drive/MyDrive/Colab Notebooks/Dataset_Melspectrograms/ChildrenPlayTrimmed'\n",
        "output_folder = '/content/drive/MyDrive/Colab Notebooks/Dataset_Melspectrograms/ChildrenPlayTrimmed_Augmented'\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process all normal .npy files in the folder\n",
        "for file in os.listdir(normal_data_folder):\n",
        "    if file.endswith('.npy'):\n",
        "        file_path = os.path.join(normal_data_folder, file)\n",
        "        create_augmentations(file_path, output_folder, sr=22050, augmentations=3)"
      ],
      "metadata": {
        "id": "YXYsITzYtIoj"
      },
      "id": "YXYsITzYtIoj",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "JL-Uu0lgahes",
      "metadata": {
        "id": "JL-Uu0lgahes"
      },
      "source": [
        "## Build selection dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bd36ae",
      "metadata": {
        "id": "18bd36ae"
      },
      "source": [
        "The spectrogram are stored in the folder `Features/melspec_313_128/`. Each machine has its folder `fan/`, `valve/` etc... <br />\n",
        "Each audio sample has its own mespectrogram flattened stored as a .npy file. So each file is one line of 313*128 = 40064 features.\n",
        "\n",
        "We first build a dataframe df which gives us all the properties of the files. This will be used to select sounds according to their section, sound_type, etc ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "Wl7ZFXd3igl3",
      "metadata": {
        "id": "Wl7ZFXd3igl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b2f2b9-b432-4aee-db7d-e5abb3b21d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame:\n",
            "                                            filepath   type    dir sound_type  \\\n",
            "0  /content/drive/MyDrive/Colab Notebooks/Dataset...  train  train     normal   \n",
            "1  /content/drive/MyDrive/Colab Notebooks/Dataset...   test   test     normal   \n",
            "2  /content/drive/MyDrive/Colab Notebooks/Dataset...  train  train     normal   \n",
            "3  /content/drive/MyDrive/Colab Notebooks/Dataset...  train  train     normal   \n",
            "4  /content/drive/MyDrive/Colab Notebooks/Dataset...  train  train     normal   \n",
            "\n",
            "                                                  id suffix  \n",
            "0  /content/drive/MyDrive/Colab Notebooks/Dataset...         \n",
            "1  /content/drive/MyDrive/Colab Notebooks/Dataset...         \n",
            "2  /content/drive/MyDrive/Colab Notebooks/Dataset...         \n",
            "3  /content/drive/MyDrive/Colab Notebooks/Dataset...         \n",
            "4  /content/drive/MyDrive/Colab Notebooks/Dataset...         \n",
            "\n",
            "Distribution of Data:\n",
            "dir    sound_type\n",
            "test   anomaly       172\n",
            "       normal         41\n",
            "train  normal        582\n",
            "val    normal         53\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def build_selection_dataframe(folder_path):\n",
        "    list_dict_file = []  # List of dictionaries for creating DataFrame\n",
        "\n",
        "    # Define a helper function to process files from a folder\n",
        "    def process_folder(folder_path):\n",
        "        for subdirectory, _, files in os.walk(folder_path):\n",
        "            # Extract the last part of the folder path (the folder name)\n",
        "            folder_name = os.path.basename(subdirectory)\n",
        "\n",
        "            # Determine if the folder is 'ChildrenPlayTrimmed', 'ChildrenPlayTrimmed_Augmented', or 'Conversation' (normal) or an anomaly folder\n",
        "            sound_type = 'normal' if folder_name in ['ChildrenPlayTrimmed', 'ChildrenPlayTrimmed_Augmented', 'Conversation'] else 'anomaly'\n",
        "\n",
        "            # Loop through files\n",
        "            for file in files:\n",
        "                if file.endswith('.npy'):  # Ensure we only process .npy files\n",
        "                    path_file = os.path.join(subdirectory, file)\n",
        "                    splitted_filename = file.split('_')\n",
        "\n",
        "                    # Randomly assign some normal sounds to the test set or validation set\n",
        "                    rand_val = np.random.rand()\n",
        "                    if sound_type == 'normal':\n",
        "                        if rand_val < 0.05:\n",
        "                            dir_type = 'test'\n",
        "                        elif rand_val < 0.15:\n",
        "                            dir_type = 'val'\n",
        "                        else:\n",
        "                            dir_type = 'train'\n",
        "                    else:\n",
        "                        dir_type = 'test'\n",
        "\n",
        "                    # Append dictionary to list\n",
        "                    list_dict_file.append({\n",
        "                        'filepath': path_file,\n",
        "                        'type': dir_type,  # Use 'train', 'val', or 'test' based on assignment\n",
        "                        'dir': dir_type,  # Normal for train, anomaly for test\n",
        "                        'sound_type': sound_type,  # 'normal' or 'anomaly'\n",
        "                        'id': path_file,  # Use path as unique ID\n",
        "                        'suffix': '_'.join(splitted_filename[6:]).split('.npy')[0]  # Extract suffix\n",
        "                    })\n",
        "\n",
        "    # Process the folder\n",
        "    process_folder(folder_path)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(list_dict_file)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Paths to the folder containing your data\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/Dataset_Melspectrograms'\n",
        "\n",
        "# Generate the combined DataFrame\n",
        "df = build_selection_dataframe(folder_path)\n",
        "\n",
        "# Display the generated DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Show distribution of train/val/test and normal/anomaly\n",
        "print(\"\\nDistribution of Data:\")\n",
        "print(df.groupby(['dir', 'sound_type']).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "xhL0-XYXkMrb",
        "outputId": "30b2ac8d-2568-4ce6-cc85-9e0d8bb63541"
      },
      "id": "xhL0-XYXkMrb",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type\n",
              "train    582\n",
              "test     213\n",
              "val       53\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7cc7b1",
      "metadata": {
        "id": "de7cc7b1"
      },
      "source": [
        "## Data Generator\n",
        "\n",
        "Since the datasets are quite big, let's make a data generator.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "d06f607f",
      "metadata": {
        "id": "d06f607f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def data_generator(file_list, batch_size, device='cpu'):\n",
        "\n",
        "    # Index used to go over file list\n",
        "    index = 0\n",
        "\n",
        "    # Infinite loop\n",
        "    while True:\n",
        "\n",
        "        # Case we looped over all the files\n",
        "        if (index + 1) * batch_size >= len(file_list):\n",
        "            # Reinitialize variables for a next round\n",
        "            index = 0\n",
        "\n",
        "            # Shuffle list to have different batches\n",
        "            random.shuffle(file_list)\n",
        "\n",
        "        # Get file paths for the current batch\n",
        "        file_chunk = file_list[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "        # Init data list\n",
        "        data = []\n",
        "\n",
        "        # Loop over batch files\n",
        "        for file in file_chunk:\n",
        "            # Load the data and reshape it as necessary\n",
        "            file_data = np.load(file).reshape(128, 431, 1)  # Assuming this shape is correct\n",
        "            file_data = torch.tensor(file_data, dtype=torch.float32).to(device)  # Convert to PyTorch tensor\n",
        "\n",
        "            data.append(file_data)\n",
        "\n",
        "        # Convert list to PyTorch tensor\n",
        "        data = torch.stack(data)  # Shape: (batch_size, 128, 431, 1)\n",
        "\n",
        "        yield data  # Return only data for training or inference\n",
        "\n",
        "        # Increment index\n",
        "        index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Ox3zVBMcEY2",
      "metadata": {
        "id": "4Ox3zVBMcEY2"
      },
      "source": [
        "## RUN : predict the machine and deduce the sound type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gergg_p8cSrV",
      "metadata": {
        "id": "gergg_p8cSrV"
      },
      "source": [
        "### First model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck: Fully connected layer\n",
        "        self.fc1 = nn.Linear(64 * 16 * 54, 128)  # Adjust dimensions according to your input size\n",
        "        self.fc2 = nn.Linear(128, 64 * 16 * 54)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Reshape back to feature map size\n",
        "        x = x.view(x.size(0), 64, 16, 54)\n",
        "\n",
        "        # Decode\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        # Ensure output size matches the input size\n",
        "        x = x[:, :, :128, :431]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OmZ-Uy_4Cl2F"
      },
      "id": "OmZ-Uy_4Cl2F",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def train_autoencoder(model, train_loader, val_loader, criterion, optimizer, device='cpu', epochs=20, gradient_clip=1.0, patience=5, save_path='best_model.pth'):\n",
        "    model.to(device)\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        # Training loop with tqdm\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False) as t_train:\n",
        "            for data, _ in t_train:  # Unpack data and ignore the labels\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(data)\n",
        "\n",
        "                # Compute reconstruction loss\n",
        "                loss = criterion(outputs, data)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item() * data.size(0)\n",
        "                t_train.set_postfix(train_loss=loss.item())\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        train_loss_history.append(train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False) as t_val:\n",
        "            with torch.no_grad():\n",
        "                for data, _ in t_val:\n",
        "                    data = data.to(device)\n",
        "                    outputs = model(data)\n",
        "                    loss = criterion(outputs, data)\n",
        "                    running_val_loss += loss.item() * data.size(0)\n",
        "                    t_val.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")\n",
        "\n",
        "    return train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "lkzclk9ll-Sh"
      },
      "id": "lkzclk9ll-Sh",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class UnsupervisedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepaths, labels):\n",
        "        self.filepaths = filepaths\n",
        "        self.labels = labels  # 'normal' or 'anomaly'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.load(self.filepaths[idx]).reshape(1, 128, 431).astype(np.float32)\n",
        "        label = self.labels[idx]\n",
        "        return torch.tensor(data), label\n",
        "\n",
        "# Prepare file paths and labels\n",
        "train_files = df[df['type'] == 'train']['filepath'].tolist()\n",
        "val_files = df[df['type'] == 'val']['filepath'].tolist()\n",
        "test_files = df[df['type'] == 'test']['filepath'].tolist()\n",
        "test_labels = df[df['type'] == 'test']['sound_type'].tolist()  # 'normal' or 'anomaly'\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(UnsupervisedDataset(train_files, ['normal'] * len(train_files)), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(UnsupervisedDataset(val_files, ['normal'] * len(val_files)), batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(UnsupervisedDataset(test_files, test_labels), batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "DmBlH-TkmBM7"
      },
      "id": "DmBlH-TkmBM7",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and optimizer\n",
        "model = CNN_Autoencoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model with validation and early stopping\n",
        "train_loss_history, val_loss_history = train_autoencoder(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device='cuda',\n",
        "    epochs=100,\n",
        "    patience=5,\n",
        "    save_path='best_autoencoder.pth'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj_xbOx6mLDE",
        "outputId": "2d49b24d-4825-4ecf-a087-cf2325746b97"
      },
      "id": "Sj_xbOx6mLDE",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] - Train Loss: 3819.0724, Val Loss: 3783.5139\n",
            "Best model saved at epoch 1 with validation loss: 3783.5139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100] - Train Loss: 3808.7709, Val Loss: 3777.4451\n",
            "Best model saved at epoch 2 with validation loss: 3777.4451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/100] - Train Loss: 3803.6617, Val Loss: 3772.5982\n",
            "Best model saved at epoch 3 with validation loss: 3772.5982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/100] - Train Loss: 3799.8187, Val Loss: 3770.3753\n",
            "Best model saved at epoch 4 with validation loss: 3770.3753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/100] - Train Loss: 3796.4436, Val Loss: 3768.1764\n",
            "Best model saved at epoch 5 with validation loss: 3768.1764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/100] - Train Loss: 3794.1384, Val Loss: 3765.0806\n",
            "Best model saved at epoch 6 with validation loss: 3765.0806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/100] - Train Loss: 3792.3759, Val Loss: 3763.4037\n",
            "Best model saved at epoch 7 with validation loss: 3763.4037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/100] - Train Loss: 3790.6949, Val Loss: 3762.1306\n",
            "Best model saved at epoch 8 with validation loss: 3762.1306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/100] - Train Loss: 3789.4293, Val Loss: 3761.5921\n",
            "Best model saved at epoch 9 with validation loss: 3761.5921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100] - Train Loss: 3788.4128, Val Loss: 3760.8785\n",
            "Best model saved at epoch 10 with validation loss: 3760.8785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/100] - Train Loss: 3787.7228, Val Loss: 3760.4410\n",
            "Best model saved at epoch 11 with validation loss: 3760.4410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/100] - Train Loss: 3787.1395, Val Loss: 3760.3777\n",
            "Best model saved at epoch 12 with validation loss: 3760.3777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100] - Train Loss: 3786.8407, Val Loss: 3759.9560\n",
            "Best model saved at epoch 13 with validation loss: 3759.9560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/100] - Train Loss: 3786.4598, Val Loss: 3759.7149\n",
            "Best model saved at epoch 14 with validation loss: 3759.7149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/100] - Train Loss: 3786.2511, Val Loss: 3759.5589\n",
            "Best model saved at epoch 15 with validation loss: 3759.5589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/100] - Train Loss: 3786.1062, Val Loss: 3759.3858\n",
            "Best model saved at epoch 16 with validation loss: 3759.3858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/100] - Train Loss: 3785.8761, Val Loss: 3759.0412\n",
            "Best model saved at epoch 17 with validation loss: 3759.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/100] - Train Loss: 3785.2488, Val Loss: 3758.1662\n",
            "Best model saved at epoch 18 with validation loss: 3758.1662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/100] - Train Loss: 3784.8012, Val Loss: 3757.9601\n",
            "Best model saved at epoch 19 with validation loss: 3757.9601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100] - Train Loss: 3785.3601, Val Loss: 3758.0580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/100] - Train Loss: 3784.5997, Val Loss: 3757.9064\n",
            "Best model saved at epoch 21 with validation loss: 3757.9064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/100] - Train Loss: 3784.4283, Val Loss: 3757.8473\n",
            "Best model saved at epoch 22 with validation loss: 3757.8473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/100] - Train Loss: 3784.4007, Val Loss: 3757.8421\n",
            "Best model saved at epoch 23 with validation loss: 3757.8421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/100] - Train Loss: 3784.3401, Val Loss: 3757.8192\n",
            "Best model saved at epoch 24 with validation loss: 3757.8192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/100] - Train Loss: 3784.6325, Val Loss: 3757.8064\n",
            "Best model saved at epoch 25 with validation loss: 3757.8064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/100] - Train Loss: 3784.4148, Val Loss: 3757.8051\n",
            "Best model saved at epoch 26 with validation loss: 3757.8051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/100] - Train Loss: 3784.2964, Val Loss: 3757.7884\n",
            "Best model saved at epoch 27 with validation loss: 3757.7884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/100] - Train Loss: 3784.2515, Val Loss: 3757.8009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/100] - Train Loss: 3784.2422, Val Loss: 3757.7779\n",
            "Best model saved at epoch 29 with validation loss: 3757.7779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100] - Train Loss: 3784.2456, Val Loss: 3757.7964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/100] - Train Loss: 3784.2300, Val Loss: 3757.7658\n",
            "Best model saved at epoch 31 with validation loss: 3757.7658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/100] - Train Loss: 3784.2937, Val Loss: 3757.7692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/100] - Train Loss: 3784.2281, Val Loss: 3757.7644\n",
            "Best model saved at epoch 33 with validation loss: 3757.7644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/100] - Train Loss: 3784.4835, Val Loss: 3757.7619\n",
            "Best model saved at epoch 34 with validation loss: 3757.7619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/100] - Train Loss: 3784.3138, Val Loss: 3757.7659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/100] - Train Loss: 3784.2234, Val Loss: 3757.7634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/100] - Train Loss: 3784.5772, Val Loss: 3757.7547\n",
            "Best model saved at epoch 37 with validation loss: 3757.7547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/100] - Train Loss: 3784.2936, Val Loss: 3757.7575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/100] - Train Loss: 3784.2116, Val Loss: 3757.7574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100] - Train Loss: 3784.2065, Val Loss: 3757.7559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/100] - Train Loss: 3784.2033, Val Loss: 3757.7539\n",
            "Best model saved at epoch 41 with validation loss: 3757.7539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/100] - Train Loss: 3784.2013, Val Loss: 3757.7518\n",
            "Best model saved at epoch 42 with validation loss: 3757.7518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/100] - Train Loss: 3784.2000, Val Loss: 3757.7538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/100] - Train Loss: 3784.2008, Val Loss: 3757.7504\n",
            "Best model saved at epoch 44 with validation loss: 3757.7504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/100] - Train Loss: 3784.4096, Val Loss: 3757.7513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/100] - Train Loss: 3784.3724, Val Loss: 3757.7500\n",
            "Best model saved at epoch 46 with validation loss: 3757.7500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/100] - Train Loss: 3784.3231, Val Loss: 3757.7489\n",
            "Best model saved at epoch 47 with validation loss: 3757.7489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/100] - Train Loss: 3784.2285, Val Loss: 3757.7517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/100] - Train Loss: 3784.2418, Val Loss: 3757.7498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100] - Train Loss: 3784.1975, Val Loss: 3757.7470\n",
            "Best model saved at epoch 50 with validation loss: 3757.7470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [51/100] - Train Loss: 3784.1928, Val Loss: 3757.7465\n",
            "Best model saved at epoch 51 with validation loss: 3757.7465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [52/100] - Train Loss: 3784.1937, Val Loss: 3757.7478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [53/100] - Train Loss: 3784.3612, Val Loss: 3757.7471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [54/100] - Train Loss: 3784.1943, Val Loss: 3757.7494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [55/100] - Train Loss: 3784.1948, Val Loss: 3757.7501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [56/100] - Train Loss: 3784.2037, Val Loss: 3757.7461\n",
            "Best model saved at epoch 56 with validation loss: 3757.7461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [57/100] - Train Loss: 3784.1921, Val Loss: 3757.7465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [58/100] - Train Loss: 3784.1971, Val Loss: 3757.7505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [59/100] - Train Loss: 3784.1862, Val Loss: 3757.7448\n",
            "Best model saved at epoch 59 with validation loss: 3757.7448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/100] - Train Loss: 3784.1919, Val Loss: 3757.7465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [61/100] - Train Loss: 3784.1888, Val Loss: 3757.7467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [62/100] - Train Loss: 3784.1895, Val Loss: 3757.7451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [63/100] - Train Loss: 3784.1888, Val Loss: 3757.7451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [64/100] - Train Loss: 3784.1883, Val Loss: 3757.7441\n",
            "Best model saved at epoch 64 with validation loss: 3757.7441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [65/100] - Train Loss: 3784.1915, Val Loss: 3757.7455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [66/100] - Train Loss: 3784.1876, Val Loss: 3757.7464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [67/100] - Train Loss: 3784.1874, Val Loss: 3757.7421\n",
            "Best model saved at epoch 67 with validation loss: 3757.7421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [68/100] - Train Loss: 3784.1843, Val Loss: 3757.7414\n",
            "Best model saved at epoch 68 with validation loss: 3757.7414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [69/100] - Train Loss: 3784.1895, Val Loss: 3757.7417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/100] - Train Loss: 3784.1813, Val Loss: 3757.7422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [71/100] - Train Loss: 3784.1835, Val Loss: 3757.7428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [72/100] - Train Loss: 3784.1801, Val Loss: 3757.7446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [73/100] - Train Loss: 3784.1799, Val Loss: 3757.7403\n",
            "Best model saved at epoch 73 with validation loss: 3757.7403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [74/100] - Train Loss: 3784.1820, Val Loss: 3757.7453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [75/100] - Train Loss: 3784.1816, Val Loss: 3757.7415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [76/100] - Train Loss: 3784.1792, Val Loss: 3757.7397\n",
            "Best model saved at epoch 76 with validation loss: 3757.7397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [77/100] - Train Loss: 3784.1822, Val Loss: 3757.7397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [78/100] - Train Loss: 3784.1789, Val Loss: 3757.7420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [79/100] - Train Loss: 3784.1797, Val Loss: 3757.7394\n",
            "Best model saved at epoch 79 with validation loss: 3757.7394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/100] - Train Loss: 3784.1787, Val Loss: 3757.7394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [81/100] - Train Loss: 3784.1777, Val Loss: 3757.7390\n",
            "Best model saved at epoch 81 with validation loss: 3757.7390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [82/100] - Train Loss: 3784.1777, Val Loss: 3757.7392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [83/100] - Train Loss: 3784.1775, Val Loss: 3757.7388\n",
            "Best model saved at epoch 83 with validation loss: 3757.7388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [84/100] - Train Loss: 3784.1779, Val Loss: 3757.7386\n",
            "Best model saved at epoch 84 with validation loss: 3757.7386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [85/100] - Train Loss: 3784.1804, Val Loss: 3757.7425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [86/100] - Train Loss: 3784.1784, Val Loss: 3757.7390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [87/100] - Train Loss: 3784.1783, Val Loss: 3757.7386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [88/100] - Train Loss: 3784.4454, Val Loss: 3757.7392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [89/100] - Train Loss: 3784.1935, Val Loss: 3757.7471\n",
            "Early stopping triggered at epoch 89\n",
            "Best model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r<ipython-input-99-26629c97cdd9>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(save_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def test_autoencoder(model, test_loader, device='cuda', threshold=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    reconstruction_errors = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.to(device)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "\n",
        "            # Compute reconstruction error (Mean Squared Error per sample)\n",
        "            error = torch.mean((outputs - data) ** 2, dim=[1, 2, 3])\n",
        "            reconstruction_errors.extend(error.cpu().numpy())\n",
        "\n",
        "    # If threshold is not provided, set it as the 90th percentile of errors\n",
        "    if threshold is None:\n",
        "        threshold = np.percentile(reconstruction_errors, 90)\n",
        "        print(f\"Using threshold (90th percentile): {threshold:.4f}\")\n",
        "\n",
        "    # Identify anomalies based on the threshold\n",
        "    predictions = ['anomaly' if error > threshold else 'normal' for error in reconstruction_errors]\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=['normal', 'anomaly']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions, labels=['normal', 'anomaly'])\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot the distribution of reconstruction errors\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(reconstruction_errors, bins=50, color='blue')\n",
        "    plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold: {threshold:.4f}')\n",
        "    plt.xlabel('Reconstruction Error')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title('Reconstruction Error Distribution')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return reconstruction_errors, threshold\n",
        "\n",
        "# Call the testing function\n",
        "reconstruction_errors, threshold = test_autoencoder(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "6hriWsR-rkpp",
        "outputId": "4227d9a0-1b5e-431c-8589-62184c6e36f9"
      },
      "id": "6hriWsR-rkpp",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using threshold (90th percentile): 8023.2097\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       1.00      0.13      0.23       172\n",
            "     anomaly       0.21      1.00      0.35        41\n",
            "\n",
            "    accuracy                           0.30       213\n",
            "   macro avg       0.61      0.56      0.29       213\n",
            "weighted avg       0.85      0.30      0.25       213\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 41   0]\n",
            " [150  22]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(CNN_VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Compute the flattened dimension after the encoder\n",
        "        self.flatten_dim = 64 * 16 * 54  # Adjust this based on your input size\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)\n",
        "        self.fc_decode = nn.Linear(latent_dim, self.flatten_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()  # Output values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.fc_decode(z)\n",
        "        x = x.view(-1, 64, 16, 54)\n",
        "        x = self.decoder(x)\n",
        "        return x[:, :, :128, :431]\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss (MSE) with mean reduction\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='mean')\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Normalize KL divergence by batch size\n",
        "    kl_divergence /= x.size(0)\n",
        "\n",
        "    return recon_loss + kl_divergence\n"
      ],
      "metadata": {
        "id": "Y6RMtydO3OCs"
      },
      "id": "Y6RMtydO3OCs",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def train_vae(model, train_loader, val_loader, optimizer, device='cuda', epochs=20, gradient_clip=1.0, patience=5, save_path='best_vae_model.pth'):\n",
        "    model.to(device)\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=True) as t_train:\n",
        "            for data, _ in t_train:\n",
        "                data = data.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                recon_data, mu, logvar = model(data)\n",
        "\n",
        "                # Compute VAE loss\n",
        "                loss = vae_loss(recon_data, data, mu, logvar)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item()\n",
        "                t_train.set_postfix(train_loss=loss.item())\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        train_loss_history.append(train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=True) as t_val:\n",
        "            with torch.no_grad():\n",
        "                for data, _ in t_val:\n",
        "                    data = data.to(device)\n",
        "                    recon_data, mu, logvar = model(data)\n",
        "                    loss = vae_loss(recon_data, data, mu, logvar)\n",
        "                    running_val_loss += loss.item()\n",
        "                    t_val.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")\n",
        "\n",
        "    return train_loss_history, val_loss_history\n"
      ],
      "metadata": {
        "id": "oQf7rBrk3S0E"
      },
      "id": "oQf7rBrk3S0E",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VAE model\n",
        "vae_model = CNN_VAE(latent_dim=128)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Train the VAE with the modified function\n",
        "train_loss_history, val_loss_history = train_vae(\n",
        "    vae_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device='cuda',\n",
        "    epochs=20,\n",
        "    patience=5,\n",
        "    save_path='best_vae_model.pth'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGu7pVJt3UXr",
        "outputId": "12c778cc-9e28-418a-ddc8-7383487814b5"
      },
      "id": "hGu7pVJt3UXr",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Training]: 100%|| 19/19 [00:03<00:00,  5.96it/s, train_loss=4.97e+3]\n",
            "Epoch 1/20 [Validation]: 100%|| 2/2 [00:00<00:00,  8.24it/s, val_loss=3.12e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Train Loss: 128.0569, Val Loss: 139.2510\n",
            "Best model saved at epoch 1 with validation loss: 139.2510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Training]: 100%|| 19/19 [00:03<00:00,  5.99it/s, train_loss=3.86e+3]\n",
            "Epoch 2/20 [Validation]: 100%|| 2/2 [00:00<00:00,  9.68it/s, val_loss=3.13e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20] - Train Loss: 126.5634, Val Loss: 139.4761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Training]: 100%|| 19/19 [00:02<00:00,  6.57it/s, train_loss=4.68e+3]\n",
            "Epoch 3/20 [Validation]: 100%|| 2/2 [00:00<00:00,  8.53it/s, val_loss=3.14e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20] - Train Loss: 127.5543, Val Loss: 139.7127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Training]: 100%|| 19/19 [00:02<00:00,  6.61it/s, train_loss=2.38e+3]\n",
            "Epoch 4/20 [Validation]: 100%|| 2/2 [00:00<00:00,  9.58it/s, val_loss=3.14e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20] - Train Loss: 124.2891, Val Loss: 139.7476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Training]: 100%|| 19/19 [00:03<00:00,  5.80it/s, train_loss=3.37e+3]\n",
            "Epoch 5/20 [Validation]: 100%|| 2/2 [00:00<00:00,  8.08it/s, val_loss=3.14e+3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20] - Train Loss: 125.6154, Val Loss: 139.6970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Training]: 100%|| 19/19 [00:03<00:00,  6.30it/s, train_loss=3.59e+3]\n",
            "Epoch 6/20 [Validation]: 100%|| 2/2 [00:00<00:00,  9.90it/s, val_loss=3.14e+3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20] - Train Loss: 125.9004, Val Loss: 139.7341\n",
            "Early stopping triggered at epoch 6\n",
            "Best model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-109-c279d52f2c55>:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(save_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_vae(model, test_loader, device='cuda', threshold=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    reconstruction_errors = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.to(device)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "            # Forward pass\n",
        "            recon_data, mu, logvar = model(data)\n",
        "\n",
        "            # Compute reconstruction error (Mean Squared Error per sample)\n",
        "            error = torch.mean((recon_data - data) ** 2, dim=[1, 2, 3])\n",
        "            reconstruction_errors.extend(error.cpu().numpy())\n",
        "\n",
        "    # Convert labels to numpy array\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Plot the distribution of reconstruction errors\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(reconstruction_errors, bins=50, color='blue')\n",
        "    plt.xlabel('Reconstruction Error')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title('Reconstruction Error Distribution')\n",
        "    plt.show()\n",
        "\n",
        "    # If threshold is not provided, set it as the 90th percentile of errors\n",
        "    if threshold is None:\n",
        "        threshold = np.percentile(reconstruction_errors, 90)\n",
        "        print(f\"Using threshold (90th percentile): {threshold:.4f}\")\n",
        "\n",
        "    # Identify anomalies based on the threshold\n",
        "    predictions = ['anomaly' if error > threshold else 'normal' for error in reconstruction_errors]\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, predictions, target_names=['normal', 'anomaly']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions, labels=['normal', 'anomaly'])\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return reconstruction_errors, threshold\n"
      ],
      "metadata": {
        "id": "hr2b-jdH5Raw"
      },
      "id": "hr2b-jdH5Raw",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_errors, threshold = test_vae(vae_model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "zDPMEhzf6eEC",
        "outputId": "00079b6d-8eb1-4c64-88b0-24c843d814e7"
      },
      "id": "zDPMEhzf6eEC",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using threshold (90th percentile): 8092.9188\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       1.00      0.13      0.23       172\n",
            "     anomaly       0.21      1.00      0.35        41\n",
            "\n",
            "    accuracy                           0.30       213\n",
            "   macro avg       0.61      0.56      0.29       213\n",
            "weighted avg       0.85      0.30      0.25       213\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 41   0]\n",
            " [150  22]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}